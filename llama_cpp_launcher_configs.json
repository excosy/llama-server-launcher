{
  "configs": {
    "reforger-qwen3-Q_8_th=24_tb=48_b=2048_ub=2048_ctx=131072_split=3,3,3_fa_mlock": {
      "llama_cpp_dir": "/home/rgilbreth/Desktop/llama.cpp",
      "venv_dir": "/home/rgilbreth/pytorch-venv",
      "model_path": "/media/rgilbreth/MISC-M2-4TB/Models/Text/Reforger/reforger-qwen3-Q_8.gguf",
      "cache_type_k": "f16",
      "threads": "24",
      "threads_batch": "48",
      "batch_size": "2048",
      "ubatch_size": "2048",
      "n_gpu_layers": "75",
      "no_mmap": false,
      "no_cnv": false,
      "prio": "0",
      "temperature": "0.8",
      "min_p": "0.05",
      "ctx_size": 131072,
      "seed": "-1",
      "flash_attn": true,
      "tensor_split": "3,3,3",
      "main_gpu": "0",
      "mlock": true,
      "no_kv_offload": false,
      "host": "127.0.0.1",
      "port": "8080",
      "ignore_eos": false,
      "n_predict": "-1",
      "template_source": "default",
      "predefined_template_name": "Let llama.cpp Decide (Use Model Default)",
      "custom_template_string": "",
      "custom_parameters": [],
      "gpu_indices": [
        0,
        1,
        2
      ]
    },
    "reforger-qwen3-Q_8_th=24_tb=48": {
      "llama_cpp_dir": "/home/rgilbreth/Desktop/llama.cpp",
      "venv_dir": "/home/rgilbreth/pytorch-venv",
      "model_path": "/media/rgilbreth/AI-M2-2TB/Models/lmstudio-community/Qwen3-32B-GGUF/Qwen3-32B-Q8_0.gguf",
      "cache_type_k": "f16",
      "threads": "24",
      "threads_batch": "48",
      "batch_size": "512",
      "ubatch_size": "512",
      "n_gpu_layers": "75",
      "no_mmap": false,
      "no_cnv": false,
      "prio": "0",
      "temperature": "0.8",
      "min_p": "0.05",
      "ctx_size": 131072,
      "seed": "-1",
      "flash_attn": true,
      "tensor_split": "3.9,4,4.1",
      "main_gpu": "1",
      "mlock": false,
      "no_kv_offload": false,
      "host": "127.0.0.1",
      "port": "8080",
      "ignore_eos": false,
      "n_predict": "-1",
      "template_source": "default",
      "predefined_template_name": "Let llama.cpp Decide (Use Model Default)",
      "custom_template_string": "",
      "custom_parameters": [
        "--top-p 0.95",
        "--top-p 0.95"
      ],
      "gpu_indices": [
        0,
        1,
        2
      ]
    },
    "qWEN3_Q8": {
      "llama_cpp_dir": "/home/rgilbreth/Desktop/llama.cpp",
      "venv_dir": "/home/rgilbreth/pytorch-venv",
      "model_path": "/media/rgilbreth/AI-M2-2TB/Models/lmstudio-community/Qwen3-32B-GGUF/Qwen3-32B-Q8_0.gguf",
      "cache_type_k": "f16",
      "threads": "24",
      "threads_batch": "48",
      "batch_size": "1024",
      "ubatch_size": "1024",
      "n_gpu_layers": "75",
      "no_mmap": false,
      "no_cnv": false,
      "prio": "0",
      "temperature": "0.6",
      "min_p": "0.0",
      "ctx_size": 131072,
      "seed": "-1",
      "flash_attn": true,
      "tensor_split": "3.9,4,4.1",
      "main_gpu": "1",
      "mlock": false,
      "no_kv_offload": false,
      "host": "127.0.0.1",
      "port": "5001",
      "ignore_eos": false,
      "n_predict": "-1",
      "template_source": "default",
      "predefined_template_name": "Let llama.cpp Decide (Use Model Default)",
      "custom_template_string": "",
      "custom_parameters": [
        "--top-p 0.95",
        "--top-p 0.95"
      ],
      "gpu_indices": [
        0,
        1,
        2
      ]
    },
    "scout": {
      "llama_cpp_dir": "/home/rgilbreth/Desktop/AI-Software/llama.cpp",
      "venv_dir": "",
      "model_path": "/media/rgilbreth/MISC-M2-4TB/Models/Text/Llama-4-Scout-17B-16E-Instruct-GGUF/Llama-4-Scout-17B-16E-Instruct-Q4_K_M-00001-of-00002.gguf",
      "cache_type_k": "q8_0",
      "cache_type_v": "q8_0",
      "threads": "24",
      "threads_batch": "48",
      "batch_size": "1024",
      "ubatch_size": "1024",
      "n_gpu_layers": "75",
      "no_mmap": false,
      "no_cnv": false,
      "prio": "0",
      "temperature": "0.6",
      "min_p": "0.0",
      "ctx_size": 32000,
      "seed": "-1",
      "flash_attn": true,
      "tensor_split": "3.9,4,4.1",
      "main_gpu": "1",
      "mlock": false,
      "no_kv_offload": false,
      "host": "127.0.0.1",
      "port": "5001",
      "ignore_eos": false,
      "n_predict": "-1",
      "template_source": "default",
      "predefined_template_name": "Let llama.cpp Decide (Use Model Default)",
      "custom_template_string": "",
      "custom_parameters": [],
      "gpu_indices": [
        0,
        1,
        2
      ]
    },
    "Qwen 235B 32k": {
      "llama_cpp_dir": "/home/rgilbreth/Desktop/llama.cpp",
      "venv_dir": "/home/rgilbreth/pytorch-venv",
      "model_path": "/media/rgilbreth/AI-M2-2TB/Models/unsloth/Qwen3-235B-A22B-128K-GGUF/Q4/Qwen3-235B-A22B-128K-Q4_0-00001-of-00003.gguf",
      "cache_type_k": "q8_0",
      "cache_type_v": "q8_0",
      "threads": "24",
      "threads_batch": "48",
      "batch_size": "1024",
      "ubatch_size": "1024",
      "n_gpu_layers": "64",
      "no_mmap": false,
      "no_cnv": false,
      "prio": "0",
      "temperature": "0.6",
      "min_p": "0.0",
      "ctx_size": 32000,
      "seed": "-1",
      "flash_attn": true,
      "tensor_split": "3.9,4,4.1",
      "main_gpu": "1",
      "mlock": false,
      "no_kv_offload": false,
      "host": "127.0.0.1",
      "port": "5001",
      "ignore_eos": false,
      "n_predict": "-1",
      "template_source": "default",
      "predefined_template_name": "Let llama.cpp Decide (Use Model Default)",
      "custom_template_string": "",
      "custom_parameters": [
        "--top-p 0.95",
        "--top-p 0.95"
      ],
      "gpu_indices": [
        0,
        1,
        2
      ]
    },
    "Qwen 235B 32k q3": {
      "llama_cpp_dir": "/home/rgilbreth/Desktop/llama.cpp",
      "venv_dir": "/home/rgilbreth/pytorch-venv",
      "model_path": "/media/rgilbreth/AI-M2-2TB/Models/unsloth/Qwen3-235B-A22B-128K-GGUF/Q3_K_S/Qwen3-235B-A22B-Q3_K_S-00001-of-00003.gguf",
      "cache_type_k": "q8_0",
      "cache_type_v": "q8_0",
      "threads": "24",
      "threads_batch": "48",
      "batch_size": "1024",
      "ubatch_size": "1024",
      "n_gpu_layers": "84",
      "no_mmap": false,
      "no_cnv": false,
      "prio": "0",
      "temperature": "0.6",
      "min_p": "0.0",
      "ctx_size": 32000,
      "seed": "-1",
      "flash_attn": true,
      "tensor_split": "3.9,4,4.1",
      "main_gpu": "1",
      "mlock": false,
      "no_kv_offload": false,
      "host": "127.0.0.1",
      "port": "5001",
      "ignore_eos": false,
      "n_predict": "-1",
      "template_source": "default",
      "predefined_template_name": "Let llama.cpp Decide (Use Model Default)",
      "custom_template_string": "",
      "custom_parameters": [
        "--top-p 0.95",
        "--top-p 0.95"
      ],
      "gpu_indices": [
        0,
        1,
        2
      ]
    },
    "Qwen 235B 32k q3 - external": {
      "llama_cpp_dir": "/home/rgilbreth/Desktop/llama.cpp",
      "venv_dir": "/home/rgilbreth/pytorch-venv",
      "model_path": "/media/rgilbreth/AI-M2-2TB/Models/unsloth/Qwen3-235B-A22B-128K-GGUF/Q3_K_S/Qwen3-235B-A22B-Q3_K_S-00001-of-00003.gguf",
      "cache_type_k": "q8_0",
      "cache_type_v": "q8_0",
      "threads": "24",
      "threads_batch": "48",
      "batch_size": "1024",
      "ubatch_size": "1024",
      "n_gpu_layers": "84",
      "no_mmap": false,
      "no_cnv": false,
      "prio": "0",
      "temperature": "0.6",
      "min_p": "0.0",
      "ctx_size": 32000,
      "seed": "-1",
      "flash_attn": true,
      "tensor_split": "3.9,4,4.1",
      "main_gpu": "1",
      "mlock": false,
      "no_kv_offload": false,
      "host": "0.0.0.0",
      "port": "5001",
      "ignore_eos": false,
      "n_predict": "-1",
      "template_source": "default",
      "predefined_template_name": "Let llama.cpp Decide (Use Model Default)",
      "custom_template_string": "",
      "custom_parameters": [
        "--top-p 0.95",
        "--api-key abcd1234ABCD5678"
      ],
      "gpu_indices": [
        0,
        1,
        2
      ]
    },
    "Qwen 235B q3 - external - 63k q_4 kv  512": {
      "llama_cpp_dir": "/home/rgilbreth/Desktop/AI-Software/llama.cpp",
      "venv_dir": "/home/rgilbreth/pytorch-venv",
      "model_path": "/media/rgilbreth/AI-M2-2TB/Models/unsloth/Qwen3-235B-A22B-128K-GGUF/Q3_K_S/Qwen3-235B-A22B-Q3_K_S-00001-of-00003.gguf",
      "cache_type_k": "q4_0",
      "cache_type_v": "q4_0",
      "threads": "24",
      "threads_batch": "48",
      "batch_size": "512",
      "ubatch_size": "512",
      "n_gpu_layers": "84",
      "no_mmap": false,
      "no_cnv": false,
      "prio": "0",
      "temperature": "0.6",
      "min_p": "0.0",
      "ctx_size": 63000,
      "seed": "-1",
      "flash_attn": true,
      "tensor_split": "3.9,4,4.1",
      "main_gpu": "1",
      "mlock": false,
      "no_kv_offload": false,
      "host": "0.0.0.0",
      "port": "5001",
      "ignore_eos": false,
      "n_predict": "-1",
      "template_source": "default",
      "predefined_template_name": "Let llama.cpp Decide (Use Model Default)",
      "custom_template_string": "",
      "custom_parameters": [
        "--top-p 0.95",
        "--api-key abcd1234ABCD5678"
      ],
      "gpu_indices": [
        0,
        1,
        2
      ],
      "environmental_variables": {
        "enabled": false,
        "predefined": {
          "GGML_CUDA_FORCE_MMQ": {
            "enabled": false,
            "value": "1"
          },
          "GGML_CUDA_F16": {
            "enabled": false,
            "value": "1"
          },
          "GGML_CUDA_GRAPH_FORCE": {
            "enabled": false,
            "value": "1"
          },
          "GGML_CUDA_FORCE_CUBLAS": {
            "enabled": false,
            "value": "1"
          },
          "GGML_CUDA_DMMV_F16": {
            "enabled": false,
            "value": "1"
          },
          "GGML_CUDA_ALLOW_FP16_REDUCE": {
            "enabled": false,
            "value": "1"
          }
        },
        "custom": []
      }
    },
    "Qwen3-235B-A22B-Q3_K_S_th=24_tb=48 env": {
      "llama_cpp_dir": "/home/rgilbreth/Desktop/AI-Software/llama.cpp",
      "venv_dir": "/home/rgilbreth/pytorch-venv",
      "model_path": "/media/rgilbreth/AI-M2-2TB/Models/unsloth/Qwen3-235B-A22B-128K-GGUF/Q3_K_S/Qwen3-235B-A22B-Q3_K_S-00001-of-00003.gguf",
      "cache_type_k": "q4_0",
      "cache_type_v": "q4_0",
      "threads": "24",
      "threads_batch": "48",
      "batch_size": "512",
      "ubatch_size": "512",
      "n_gpu_layers": "84",
      "no_mmap": false,
      "no_cnv": false,
      "prio": "0",
      "temperature": "0.6",
      "min_p": "0.0",
      "ctx_size": 63000,
      "seed": "-1",
      "flash_attn": true,
      "tensor_split": "3.9,4,4.1",
      "main_gpu": "1",
      "mlock": false,
      "no_kv_offload": false,
      "host": "0.0.0.0",
      "port": "5001",
      "ignore_eos": false,
      "n_predict": "-1",
      "template_source": "default",
      "predefined_template_name": "Let llama.cpp Decide (Use Model Default)",
      "custom_template_string": "",
      "custom_parameters": [
        "--top-p 0.95",
        "--api-key abcd1234ABCD5678"
      ],
      "gpu_indices": [
        0,
        1,
        2
      ],
      "environmental_variables": {
        "enabled": true,
        "predefined": {
          "GGML_CUDA_FORCE_MMQ": {
            "enabled": false,
            "value": "1"
          },
          "GGML_CUDA_F16": {
            "enabled": false,
            "value": "1"
          },
          "GGML_CUDA_GRAPH_FORCE": {
            "enabled": false,
            "value": "1"
          },
          "GGML_CUDA_FORCE_CUBLAS": {
            "enabled": false,
            "value": "1"
          },
          "GGML_CUDA_DMMV_F16": {
            "enabled": false,
            "value": "1"
          },
          "GGML_CUDA_ALLOW_FP16_REDUCE": {
            "enabled": false,
            "value": "1"
          }
        },
        "custom": []
      }
    }
  },
  "app_settings": {
    "last_llama_cpp_dir": "",
    "last_venv_dir": "/home/rgilbreth/pytorch-venv",
    "last_model_path": "/media/rgilbreth/AI-M2-2TB/Models/unsloth/Qwen3-235B-A22B-128K-GGUF/Q3_K_S/Qwen3-235B-A22B-Q3_K_S-00001-of-00003.gguf",
    "model_dirs": [
      "/media/rgilbreth/MISC-M2-4TB/Models/Text",
      "/media/rgilbreth/AI-M2-2TB/Models"
    ],
    "model_list_height": 8,
    "selected_gpus": [
      0,
      1,
      2
    ],
    "host": "0.0.0.0",
    "port": "5001",
    "manual_gpu_mode": false,
    "manual_gpu_count": "1",
    "manual_gpu_vram": "8.0",
    "manual_model_mode": false,
    "manual_model_layers": "32",
    "manual_model_size_gb": "7.0",
    "custom_parameters": [
      "--top-p 0.95",
      "--api-key abcd1234ABCD5678"
    ]
  }
}